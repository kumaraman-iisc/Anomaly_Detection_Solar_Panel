# -*- coding: utf-8 -*-
"""Quantization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FZfP7-mN9TpCa1oBf9lERqGyiQitnQVK

# Batch Initiation
"""

def representative_data_gen():
    for _ in range(100):
        images, _ = next(train_generator)  # Use next() to get a batch
        yield [tf.cast(images, tf.float32)]

"""## For TensorFlow model files"""

converter = tf.lite.TFLiteConverter.from_saved_model("saved_model")
# Replace saved_model --> mobilenet_saved_model for mobilenet
#                     --> tinycnn_saved_model for TinyCNN
#                     --> resnetx_saved_model for Resnetx
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen

converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
converter.inference_input_type = tf.uint8
converter.inference_output_type = tf.uint8

tflite_model = converter.convert()

with open("modelname_int8_quant.tflite", "wb") as f:      # Replace modelname_int8_quant.tflite with appropriate name
    f.write(tflite_model)

"""## For Keras"""

# Load the keras model
model = tf.keras.models.load_model("name_model.keras")  # Replace name_model.keras with model name

converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.representative_dataset = representative_data_gen
converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]

converter.inference_input_type = tf.int8
converter.inference_output_type = tf.int8

tflite_model = converter.convert()

with open("modelname_keras_int8.tflite", "wb") as f:
    f.write(tflite_model)